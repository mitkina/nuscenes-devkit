{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "broken-blake",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from nuscenes.eval.prediction.splits import get_prediction_challenge_split\n",
    "\n",
    "from nuscenes.prediction.models.backbone import ResNetBackbone\n",
    "from nuscenes.prediction.models.mtp import MTP\n",
    "from nuscenes.prediction.models.covernet import CoverNet\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from nuscenes.prediction.input_representation.static_layers import StaticLayerRasterizer\n",
    "from nuscenes.prediction.input_representation.agents import AgentBoxesWithFadedHistory\n",
    "from nuscenes.prediction.input_representation.interface import InputRepresentation\n",
    "from nuscenes.prediction.input_representation.combinators import Rasterizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "quarterly-delivery",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictionModel(pl.LightningModule):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        backbone = ResNetBackbone('resnet50')\n",
    "        \n",
    "        # Note that the value of num_modes depends on the size of the lattice used for CoverNet.\n",
    "        self.covernet = CoverNet(backbone, num_modes=64)\n",
    "        # mtp = MTP(backbone, num_modes=2)\n",
    "        \n",
    "        # How to pass the nuscenes data helper to the model module?\n",
    "        self.helper = PredictHelper(nuscenes)\n",
    "        \n",
    "        # Image representation\n",
    "        static_layer_rasterizer = StaticLayerRasterizer(helper)\n",
    "        agent_rasterizer = AgentBoxesWithFadedHistory(helper, seconds_of_history=1)\n",
    "        self.mtp_input_representation = InputRepresentation(static_layer_rasterizer, agent_rasterizer, Rasterizer())\n",
    "\n",
    "    def forward(self, x):\n",
    "        # in lightning, forward defines the prediction/inference actions\n",
    "        logits = self.covernet(x)\n",
    "        return logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defined the train loop.\n",
    "        # It is independent of forward\n",
    "        x, y = batch\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # Needs to be set by the batch\n",
    "        instance_token_2, sample_token_2 = mini_train[5].split(\"_\")\n",
    "        instance_token_img, sample_token_img = 'bc38961ca0ac4b14ab90e547ba79fbb6', '7626dde27d604ac28a0240bdd54eba7a'\n",
    "        img = self.mtp_input_representation.make_input_representation(instance_token_img, sample_token_img)\n",
    "        \n",
    "        agent_state_vector = torch.Tensor([[helper.get_velocity_for_agent(instance_token_img, sample_token_img),\n",
    "                                    helper.get_acceleration_for_agent(instance_token_img, sample_token_img),\n",
    "                                    helper.get_heading_change_rate_for_agent(instance_token_img, sample_token_img)]])\n",
    "\n",
    "        image_tensor = torch.Tensor(img).permute(2, 0, 1).unsqueeze(0)\n",
    "        \n",
    "        logits = self.covernet(image_tensor, agent_state_vector)\n",
    "        loss = F.mse_loss(x_hat, x)\n",
    "        # Logging to TensorBoard by default\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "overhead-correlation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /home/mitkina/git-repos/Waymo/nuscenes-devkit/python-sdk/tutorials/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "090642d8fc594ad5a262e97c588eabfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/mitkina/git-repos/Waymo/nuscenes-devkit/python-sdk/tutorials/MNIST/raw/train-images-idx3-ubyte.gz to /home/mitkina/git-repos/Waymo/nuscenes-devkit/python-sdk/tutorials/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to /home/mitkina/git-repos/Waymo/nuscenes-devkit/python-sdk/tutorials/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5367330972334a53ac5146261b90e96b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/mitkina/git-repos/Waymo/nuscenes-devkit/python-sdk/tutorials/MNIST/raw/train-labels-idx1-ubyte.gz to /home/mitkina/git-repos/Waymo/nuscenes-devkit/python-sdk/tutorials/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to /home/mitkina/git-repos/Waymo/nuscenes-devkit/python-sdk/tutorials/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e04214b5f3b46b982ecf023e269f0ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/mitkina/git-repos/Waymo/nuscenes-devkit/python-sdk/tutorials/MNIST/raw/t10k-images-idx3-ubyte.gz to /home/mitkina/git-repos/Waymo/nuscenes-devkit/python-sdk/tutorials/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to /home/mitkina/git-repos/Waymo/nuscenes-devkit/python-sdk/tutorials/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6cd74b0f57d4947b51571d14fafdad8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/mitkina/git-repos/Waymo/nuscenes-devkit/python-sdk/tutorials/MNIST/raw/t10k-labels-idx1-ubyte.gz to /home/mitkina/git-repos/Waymo/nuscenes-devkit/python-sdk/tutorials/MNIST/raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mitkina/anaconda3/envs/bp/lib/python3.6/site-packages/torchvision/datasets/mnist.py:480: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "DATAROOT = '/home/mitkina/git-repos/Waymo/data/sets/nuscenes'\n",
    "\n",
    "# nuscenes = NuScenes('v1.0-mini', dataroot=DATAROOT)\n",
    "nuscenes = NuScenes('v1.0-trainval', dataroot=DATAROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transsexual-separation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = MNIST(os.getcwd(), download=True, transform=transforms.ToTensor())\n",
    "\n",
    "train_dataset = get_prediction_challenge_split(\"train\", dataroot=DATAROOT)\n",
    "val_dataset = get_prediction_challenge_split(\"validation\", dataroot=DATAROOT)\n",
    "\n",
    "train_loader = DataLoader(dataset)\n",
    "\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=input_cfg.batch_size * num_gpu,\n",
    "        shuffle=True,\n",
    "        num_workers=input_cfg.preprocess.num_workers * num_gpu,\n",
    "        pin_memory=False,\n",
    "        collate_fn=collate_fn,\n",
    "        worker_init_fn=_worker_init_fn,\n",
    "        drop_last=not multi_gpu)\n",
    "    eval_dataloader = torch.utils.data.DataLoader(\n",
    "        eval_dataset,\n",
    "        batch_size=eval_input_cfg.batch_size, # only support multi-gpu train\n",
    "        shuffle=False,\n",
    "        num_workers=eval_input_cfg.preprocess.num_workers,\n",
    "        pin_memory=False,\n",
    "        collate_fn=merge_second_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stretch-dover",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type       | Params\n",
      "---------------------------------------\n",
      "0 | encoder | Sequential | 50.4 K\n",
      "1 | decoder | Sequential | 51.2 K\n",
      "---------------------------------------\n",
      "101 K     Trainable params\n",
      "0         Non-trainable params\n",
      "101 K     Total params\n",
      "0.407     Total estimated model params size (MB)\n",
      "/home/mitkina/anaconda3/envs/bp/lib/python3.6/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88cfc8c929cd4c2ebdb6d50dd8eaf6c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# init model\n",
    "autoencoder = LitAutoEncoder()\n",
    "\n",
    "# most basic trainer, uses good defaults (auto-tensorboard, checkpoints, logs, and more)\n",
    "# trainer = pl.Trainer(gpus=8) (if you have GPUs)\n",
    "trainer = pl.Trainer(gpus=1)\n",
    "trainer.fit(autoencoder, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demographic-lincoln",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "trainer.test(datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "global-miller",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LitModel.load_from_checkpoint(path)\n",
    "\n",
    "# # load the ckpt\n",
    "# ckpt = torch.load('path/to/checkpoint.ckpt')\n",
    "\n",
    "# # equivalent to the above\n",
    "# model = LitModel()\n",
    "# model.load_state_dict(ckpt['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "common-surgeon",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NuScenesDataModule(LightningDataModule):\n",
    "\n",
    "      def __init__(self, batch_size=32, dataroot):\n",
    "          super().__init__()\n",
    "          self.batch_size = batch_size\n",
    "          self.dataroot = dataroot\n",
    "            \n",
    "          self.nuscenes = NuScenes('v1.0-trainval', dataroot=self.dataroot)\n",
    "          self.helper = PredictHelper(self.nuscenes)\n",
    "\n",
    "      # When doing distributed training, Datamodules have two optional arguments for\n",
    "      # granular control over download/prepare/splitting data:\n",
    "\n",
    "      # OPTIONAL, called only on 1 GPU/machine\n",
    "      # write to disk\n",
    "#       def prepare_data(self):\n",
    "#           MNIST(os.getcwd(), train=True, download=True)\n",
    "#           MNIST(os.getcwd(), train=False, download=True)\n",
    "\n",
    "      # OPTIONAL, called for every GPU/machine (assigning state is OK)\n",
    "      def setup(self, stage: Optional[str] = None):\n",
    "          # transforms\n",
    "          transform=transforms.Compose([\n",
    "              transforms.ToTensor(),\n",
    "              transforms.Normalize((0.1307,), (0.3081,))\n",
    "          ])\n",
    "          # split dataset\n",
    "          if stage == 'fit':\n",
    "              train = get_prediction_challenge_split(\"train\", dataroot=self.dataroot)\n",
    "              self.train, self.val = random_split(train, [int(0.9*len(train), len(train) - int(0.9*len(train)])\n",
    "#               mnist_train = MNIST(os.getcwd(), train=True, transform=transform)\n",
    "#               self.mnist_train, self.mnist_val = random_split(mnist_train, [55000, 5000])\n",
    "# #           if stage == 'validate':\n",
    "# #               mnist_train = MNIST(os.getcwd(), train=True, transform=transform)\n",
    "#               self.mnist_train, self.mnist_val = random_split(mnist_train, [55000, 5000])\n",
    "          if stage == 'test':\n",
    "#               self.mnist_test = MNIST(os.getcwd(), train=False, transform=transform)\n",
    "                self.test = get_prediction_challenge_split(\"val\", dataroot=self.dataroot)\n",
    "#           if stage == 'predict':\n",
    "#               self.mnist_test = MNIST(os.getcwd(), train=False, transform=transform)\n",
    "\n",
    "      # return the dataloader for each split\n",
    "      def train_dataloader(self):\n",
    "          mnist_train = DataLoader(self.train, batch_size=self.batch_size)\n",
    "          return mnist_train\n",
    "\n",
    "      def val_dataloader(self):\n",
    "          mnist_val = DataLoader(self.val, batch_size=self.batch_size)\n",
    "          return mnist_val\n",
    "\n",
    "      def test_dataloader(self):\n",
    "          mnist_test = DataLoader(self.test, batch_size=self.batch_size)\n",
    "          return mnist_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
